{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.13 (main, Aug 25 2022, 23:26:10) \\n[GCC 11.2.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#math\n",
    "import numpy as np\n",
    "#capture\n",
    "import cv2\n",
    "import wave\n",
    "#utils\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disable CUDA devices\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.2\n"
     ]
    }
   ],
   "source": [
    "#neural\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "system info\n",
    "-"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU|/device:CPU:0 memory limit: 268435456\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 03:17:02.204353: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-15 03:17:02.211222: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-10-15 03:17:02.211261: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: A-WS-M21\n",
      "2022-10-15 03:17:02.211270: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: A-WS-M21\n",
      "2022-10-15 03:17:02.211345: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.76.0\n",
      "2022-10-15 03:17:02.211371: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.76.0\n",
      "2022-10-15 03:17:02.211382: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.76.0\n"
     ]
    }
   ],
   "source": [
    "for i in device_lib.list_local_devices():\n",
    "    print(f'{i.device_type}|{i.name} memory limit: {i.memory_limit}')\n",
    "    print(i.physical_device_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import data\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape(frame):\n",
    "    global WIDTH, HEIGHT\n",
    "    newframe = np.zeros(WIDTH*HEIGHT).reshape(HEIGHT, WIDTH)\n",
    "    if frame.shape[0] == HEIGHT and frame.shape[1] == WIDTH:\n",
    "        return frame\n",
    "    elif frame.shape[0] <= HEIGHT and frame.shape[1] <= WIDTH:\n",
    "        newframe[:frame.shape[0],:frame.shape[1]] = frame\n",
    "    elif frame.shape[0] <= HEIGHT:\n",
    "        newframe[:frame.shape[0],:] = frame[:, :WIDTH]\n",
    "    elif frame.shape[1] <= WIDTH:\n",
    "        newframe[:,:frame.shape[1]] = frame[:HEIGHT, :]\n",
    "    else:\n",
    "        newframe = frame[:HEIGHT, :WIDTH]\n",
    "    return newframe\n",
    "\n",
    "def deinterlase(frame):\n",
    "    even_fields=frame[::2,:];\n",
    "    odd_fields=frame[1::2,:];\n",
    "    return even_fields, odd_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "WIDTH = int(720)\n",
    "HEIGHT = int(625-1)\n",
    "\n",
    "def normilizeFrame(frame):\n",
    "    global WIDTH, HEIGHT\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #change dimentions(choose 1 of 2 options)\n",
    "    #frame = cv2.resize(frame, (WIDTH, HEIGHT), interpolation=cv2.INTER_NEAREST)\n",
    "    frame = reshape(frame)\n",
    "    \n",
    "    return deinterlase(frame)\n",
    "    #return frame / 255.0\n",
    "\n",
    "def retrieve():\n",
    "    global WIDTH, HEIGHT\n",
    "    if cap.grab():\n",
    "        ret, frame = cap.retrieve()\n",
    "        if not ret:\n",
    "            print('error reading frame')\n",
    "            return False, normilizeFrame(np.zeros(WIDTH*HEIGHT).reshape(WIDTH, HEIGHT))\n",
    "        return True, normilizeFrame(frame)\n",
    "    else:\n",
    "        return False, normilizeFrame(np.zeros(WIDTH*HEIGHT).reshape(WIDTH, HEIGHT))\n",
    "\n",
    "def getNext():\n",
    "    global first, topFrame, bottomFrame, half\n",
    "    if first == True:\n",
    "        first = False\n",
    "        ret, (topFrame, bottomFrame) = retrieve()\n",
    "        return ret, cv2.vconcat([topFrame, bottomFrame])[np.newaxis is None,:,:]\n",
    "    \n",
    "    if half:\n",
    "        currTop = bottomFrame\n",
    "        ret, (topFrame, bottomFrame) = retrieve()\n",
    "        half = False\n",
    "        return ret, cv2.vconcat([currTop, topFrame])[np.newaxis is None,:,:]\n",
    "    else:\n",
    "        half = True\n",
    "        return True, cv2.vconcat([topFrame, bottomFrame])[np.newaxis is None,:,:]\n",
    "    \n",
    "fileName = 'testfiles/pal/whitenosie'\n",
    "\n",
    "def openCap(file):\n",
    "    global cap, gray, half, first\n",
    "    half = True\n",
    "    first = True\n",
    "    cap = cv2.VideoCapture(file + '.avi')\n",
    "    if (cap.isOpened()== False): \n",
    "      print(\"Error opening video stream or file\")\n",
    "\n",
    "openCap(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7501\n",
      "882\n"
     ]
    }
   ],
   "source": [
    "frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(frameCount)\n",
    "\n",
    "#tech calc\n",
    "\n",
    "#audio in pcm have 44100 samples in second per channel, respectively we have 88200 samples\n",
    "#per frame we have 44100/25(pal) = 1764 samples(for single channel)(and we take half of frame for learning /2)\n",
    "\n",
    "samplesPerFrame = int(44100/cap.get(cv2.CAP_PROP_FPS)/2)\n",
    "print(samplesPerFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./dataset/\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "!rm -rf ./dataset\n",
    "!mkdir -p dataset\n",
    "!mkdir -p dataset/cache\n",
    "\n",
    "import shutil\n",
    "shutil.copyfile(fileName + '.avi', f'{dir}video.avi')\n",
    "shutil.copyfile(fileName + '.wav', f'{dir}audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainAudioFile = wave.open(f'{dir}audio.wav', mode='r')\n",
    "\n",
    "def gen():\n",
    "    global mainAudioFile, samplesPerFrame\n",
    "    currentFrame = 1\n",
    "    \n",
    "    #audio\n",
    "    mainAudioFile.rewind()\n",
    "    \n",
    "    if not (mainAudioFile.getparams()[0] == 2 and mainAudioFile.getparams()[2] == 44100):\n",
    "        print('wrong audiofile params')\n",
    "        sys.exit(1)\n",
    "    \n",
    "    #video\n",
    "    \n",
    "    openCap(fileName)\n",
    "    frames = cap.get(cv2.CAP_PROP_FRAME_COUNT) -1\n",
    "    \n",
    "    while currentFrame < frames:\n",
    "        #audio\n",
    "        frame = mainAudioFile.readframes(samplesPerFrame)\n",
    "        \n",
    "        if len(frame) < samplesPerFrame*2:\n",
    "            frame = frame + b'\\x00' * ((samplesPerFrame*4)-len(frame))\n",
    "        \n",
    "        #video\n",
    "        vframe = getNext()[1]\n",
    "        \n",
    "        yield vframe, tf.io.decode_raw(frame, tf.int16, little_endian=True).numpy()/16383.0\n",
    "        currentFrame = currentFrame+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=tf.data.Dataset.from_generator(\n",
    "    gen, \n",
    "    output_signature=(tf.TensorSpec(shape=(1, HEIGHT, WIDTH), dtype=tf.uint8, name='image'),\n",
    "                      tf.TensorSpec(shape=(samplesPerFrame*2,), dtype=tf.int16, name='audio'),\n",
    "                      #tf.TensorSpec(shape=(1), dtype=tf.int32, name='frameNum')\n",
    "                     )\n",
    ")#.cache('./dataset/cache/')#.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#create model\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = keras.Input(shape=((HEIGHT), WIDTH), name = 'image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 624, 720])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Rescaling(1./127.5, name='normalize', offset=-1)(input)\n",
    "x = tf.keras.layers.Reshape((x.shape[1], x.shape[2], 1))(x)\n",
    "filter = 8\n",
    "sizekern = 64\n",
    "\n",
    "i=1\n",
    "x = tf.keras.layers.Conv2D(filter, (4, sizekern), activation='relu', name=f'conv2d_{i}')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(name=f'MaxPooling2D_{i}')(x)\n",
    "\n",
    "filter = 1\n",
    "sizekern = 256\n",
    "\n",
    "i=2\n",
    "x = tf.keras.layers.Conv2D(filter, (2, sizekern), activation='relu', name=f'conv2d_{i}')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(name=f'MaxPooling2D_{i}')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 154, 36, 1)\n",
      "5544\n",
      "diff 3780\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x.shape[1]*x.shape[2]*x.shape[3])\n",
    "print(f'diff {x.shape[1]*x.shape[2]*x.shape[3]-(samplesPerFrame*2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.layers.Flatten(name='Flatten')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#x = tf.keras.layers.Dense((samplesPerFrame*2), activation='linear', bias_initializer='ones', use_bias=True, name='layer_1')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = tf.keras.layers.Dense((samplesPerFrame*2), activation='sigmoid', bias_initializer='ones', use_bias=True, name='audio')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " image (InputLayer)          [(None, 624, 720)]        0         \n",
      "                                                                 \n",
      " normalize (Rescaling)       (None, 624, 720)          0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 624, 720, 1)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 621, 657, 8)       2056      \n",
      "                                                                 \n",
      " MaxPooling2D_1 (MaxPooling2  (None, 310, 328, 8)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 309, 73, 1)        4097      \n",
      "                                                                 \n",
      " MaxPooling2D_2 (MaxPooling2  (None, 154, 36, 1)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 5544)              0         \n",
      "                                                                 \n",
      " audio (Dense)               (None, 1764)              9781380   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,787,533\n",
      "Trainable params: 9,787,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Model(input, output)\n",
    "model.summary()\n",
    "\n",
    "#tf.keras.utils.plot_model(\n",
    "#    model,\n",
    "#    to_file=\"model.png\",\n",
    "#    show_shapes=True,\n",
    "#    show_dtype=False,\n",
    "#    show_layer_names=True,\n",
    "#    rankdir=\"TB\",\n",
    "#    expand_nested=True,\n",
    "#    dpi=96,\n",
    "#    layer_range=None,\n",
    "#    show_layer_activations=True,\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compile and learn\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = 'testfiles/pal/streetSrakerffV1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   7499/Unknown - 7943s 1s/step - loss: 0.5003 - binary_accuracy: 0.5000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 05:29:26.091845: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2347107840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7499/7499 [==============================] - 7944s 1s/step - loss: 0.5003 - binary_accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "7499/7499 [==============================] - ETA: 0s - loss: 0.5002 - binary_accuracy: 0.5001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 07:40:34.860680: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2347107840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7499/7499 [==============================] - 7869s 1s/step - loss: 0.5002 - binary_accuracy: 0.5001\n",
      "Epoch 3/5\n",
      "7499/7499 [==============================] - ETA: 0s - loss: 0.5002 - binary_accuracy: 0.5001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 09:52:47.388609: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2347107840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7499/7499 [==============================] - 7933s 1s/step - loss: 0.5002 - binary_accuracy: 0.5001\n",
      "Epoch 4/5\n",
      "7499/7499 [==============================] - ETA: 0s - loss: 0.5002 - binary_accuracy: 0.5001"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 12:08:49.413714: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2347107840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7499/7499 [==============================] - 8162s 1s/step - loss: 0.5002 - binary_accuracy: 0.5001\n",
      "Epoch 5/5\n",
      "1005/7499 [===>..........................] - ETA: 1:58:10 - loss: 0.4997 - binary_accuracy: 0.5006"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = 1\n",
    "step = 1\n",
    "while loss >= 0.1:\n",
    "    fitHistory = model.fit(dataset, epochs=2, callbacks=[tensorboard_callback])\n",
    "    model.save(\"model\")\n",
    "    if step%1 == 0:\n",
    "        openCap(testfile + '.avi')\n",
    "        \n",
    "        bar = tqdm(total=cap.get(cv2.CAP_PROP_FRAME_COUNT), ncols=100)\n",
    "        file = wave.open(f'output/{step}.wav', mode='w')\n",
    "        file.setnchannels(2) \n",
    "        file.setsampwidth(2)\n",
    "        file.setframerate(44100)\n",
    "        ret, vframe = getNext()\n",
    "        while ret:\n",
    "            file.writeframes((model.predict(vframe, verbose=0)[0]*16383).astype(np.int16).tobytes())\n",
    "            bar.update(1)\n",
    "            ret, vframe = getNext()\n",
    "            if int(cap.get(cv2.CAP_PROP_POS_FRAMES)) > 25*2*15:    #limiting 30 on seconds\n",
    "                ret = False\n",
    "        bar.close()\n",
    "        file.close()\n",
    "        step = step +1\n",
    "    loss = fitHistory.history['loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('1.png', vframe.reshape(624, 720))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vframe.reshape(624, 720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d94a8b67015ef768\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d94a8b67015ef768\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
